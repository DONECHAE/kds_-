# streamlit_app.py
# ì‹¤í–‰: streamlit run streamlit_app.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import io, os, time
from pathlib import Path
from typing import List, Dict, Any
import streamlit as st
from dotenv import load_dotenv

load_dotenv()

st.set_page_config(page_title="ìº¡ì…˜â†’ìš”ì•½â†’í˜¸ê°ë„ ë¶„ë¥˜+í•´ì„ ë¦¬í¬íŠ¸", layout="wide")
st.title("ğŸ“¸ ìº¡ì…˜ 6ê°œ â†’ ğŸ“ ìš”ì•½ 1ê°œ â†’ ğŸ§  í˜¸ê°ë„ ë¶„ë¥˜ + ğŸ” í•´ì„ ë¦¬í¬íŠ¸")
st.caption("ì´ë¯¸ì§€/ë™ì˜ìƒ 1ê°œ ì—…ë¡œë“œ â†’ 6ìº¡ì…˜ ìƒì„± â†’ 1ìš”ì•½ ìƒì„± â†’ ìš”ì•½ë§Œ DeBERTa ë¶„ë¥˜ â†’ Attention/IG/LIME í•´ì„ â†’ GPT ë¦¬í¬íŠ¸(ë¹„ì „ë¬¸ê°€/ì „ë¬¸ê°€, ë‹¤êµ­ì–´)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì´ë“œë°” ì˜µì…˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.subheader("â‘  ìº¡ì…˜ ì˜µì…˜")
    pos_text    = st.text_input("í”„ë ˆì„ ë¹„ìœ¨(ì‰¼í‘œ, 0~1)", "0.10,0.20,0.45,0.55,0.80,0.90")
    caption_model  = st.text_input("ìº¡ì…˜ ëª¨ë¸", os.environ.get("GPT_CAPTION_MODEL", "gpt-4.1-mini"))
    base_temp   = st.slider("temperature", 0.0, 1.5, 0.7, 0.05)
    max_tokens  = st.number_input("max tokens", min_value=32, max_value=512, value=120, step=8)
    concurrency = st.slider("ë™ì‹œ ì²˜ë¦¬", 1, 16, 6, 1)

    st.subheader("â‘¡ ë¶„ë¥˜(DeBERTa) ëª¨ë¸")
    ckpt_dir = st.text_input("ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ", r"C:\Users\DC\2025_kds\MODULES\deberta_prompt_model_V4_test3")
    cls_bs   = st.slider("ë¶„ë¥˜ ë°°ì¹˜ í¬ê¸°", 1, 32, 8, 1)

    st.subheader("â‘¢ ë¦¬í¬íŠ¸ ì–¸ì–´")
    lang = st.selectbox(
        "ë¦¬í¬íŠ¸ ì–¸ì–´ ì„ íƒ",
        ["ko","en","fr","ja","es","ru"],
        index=0,
        format_func=lambda x: {
            "ko":"í•œêµ­ì–´","en":"English","fr":"FranÃ§ais",
            "ja":"æ—¥æœ¬èª","es":"EspaÃ±ol","ru":"Ğ ÑƒÑÑĞºĞ¸Ğ¹"
        }[x]
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—…ë¡œë” (ë‹¨ì¼ íŒŒì¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
uploaded = st.file_uploader(
    "ì´ë¯¸ì§€(.jpg/.png/...) ë˜ëŠ” ë™ì˜ìƒ(.mp4 ë“±) 1ê°œ ì—…ë¡œë“œ",
    type=["jpg","jpeg","png","webp","bmp","mp4","avi","mov","mkv","wmv","m4v"],
    accept_multiple_files=False
)

def parse_positions(text: str) -> List[float]:
    try:
        vals = [float(x.strip()) for x in text.split(",") if x.strip()]
        vals = [min(max(v, 0.0), 1.0) for v in vals]
        return vals or [0.10,0.20,0.45,0.55,0.80,0.90]
    except:
        return [0.10,0.20,0.45,0.55,0.80,0.90]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìºì‹œëœ ìº¡ì…”ë„ˆ (ë²„íŠ¼ ì´í›„ ì„í¬íŠ¸ â†’ ì´ˆê¸° ë¡œë”© ê°€ë³ê²Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def get_captioner(positions, model, base_temp, max_tokens, concurrency):
    from MODULES.image import MediaCaptioner  # lazy import
    return MediaCaptioner(
        positions=positions,
        model=model,
        base_temperature=float(base_temp),
        max_tokens=int(max_tokens),
        concurrency=int(concurrency),
        retries=5,
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìºì‹œëœ ê°„ë‹¨ ë¶„ë¥˜ê¸° (DeBERTa) - ìš”ì•½ 1ê°œë§Œ ë¶„ë¥˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def get_deberta_classifier(ckpt_dir: str):
    import torch
    from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification

    class _Cls:
        def __init__(self, model_dir: str):
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.fp16 = (self.device == "cuda")
            self.config = AutoConfig.from_pretrained(model_dir, local_files_only=True)
            self.tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True, local_files_only=True)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_dir, local_files_only=True).to(self.device).eval()
            # ë¼ë²¨: labels.txt ìš°ì„ 
            labels = None
            labels_path = os.path.join(model_dir, "labels.txt")
            if os.path.exists(labels_path):
                with open(labels_path, "r", encoding="utf-8") as f:
                    labels = [ln.strip() for ln in f if ln.strip()]
            if labels:
                self.id2label = {i: lab for i, lab in enumerate(labels)}
            elif getattr(self.config, "id2label", None):
                self.id2label = {int(k): v for k, v in self.config.id2label.items()}
            else:
                # ê¸°ë³¸(ì˜ˆì‹œ)
                self.id2label = {0: "Unlikable", 1: "Likable"}

        @torch.inference_mode()
        def predict_one(self, text: str, max_len: int = 256) -> Dict[str, Any]:
            enc = self.tok(text, padding=True, truncation=True, max_length=max_len, return_tensors="pt")
            enc = {k: v.to(self.device) for k, v in enc.items()}
            if self.fp16:
                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    logits = self.model(**enc).logits
            else:
                logits = self.model(**enc).logits
            probs = logits.softmax(dim=-1)[0].detach().float().cpu().tolist()
            pred_idx = int(torch.tensor(probs).argmax().item())
            return {
                "pred_idx": pred_idx,
                "pred_label": self.id2label.get(pred_idx, str(pred_idx)),
                "probs": probs
            }

    return _Cls(ckpt_dir)

def is_video(name: str) -> bool:
    from MODULES.image import VIDEO_EXTS  # lazy
    return Path(name).suffix.lower() in VIDEO_EXTS

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ë²„íŠ¼: ìº¡ì…˜ 6 â†’ ìš”ì•½ 1 â†’ ë¶„ë¥˜(ìš”ì•½ë§Œ) â†’ í•´ì„(Attention/IG/LIME) â†’ GPT ë¦¬í¬íŠ¸(íƒ­)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if uploaded is not None:
    file_name  = uploaded.name
    file_bytes = uploaded.read()
    st.write(f"**íŒŒì¼ëª…:** {file_name} | **í¬ê¸°:** {len(file_bytes):,} bytes")

    if not os.environ.get("OPENAI_API_KEY"):
        st.error("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        st.stop()

    if st.button("ğŸš€ ìº¡ì…˜ 6ê°œ ìƒì„± â†’ ìš”ì•½ â†’ (ìš”ì•½ë§Œ) ë¶„ë¥˜ + í•´ì„ ë¦¬í¬íŠ¸"):
        t0 = time.perf_counter()
        positions = parse_positions(pos_text)

        # ì¤€ë¹„
        mc   = get_captioner(positions, caption_model, base_temp, max_tokens, concurrency)
        clf  = get_deberta_classifier(ckpt_dir)

        # 1) ìº¡ì…˜ 6ê°œ + ìš”ì•½
        with st.spinner("ìº¡ì…˜ ìƒì„± + ìš”ì•½ ì¤‘..."):
            import asyncio
            captions, summary, previews = asyncio.run(mc.process_with_summary(file_bytes, file_name))
        st.success(f"ìº¡ì…˜ {len(captions)}ê°œ + ìš”ì•½ ìƒì„± ì™„ë£Œ (+{time.perf_counter()-t0:.2f}s)")

        st.subheader("ìº¡ì…˜ (6ê°œ)")
        for i, c in enumerate(captions, 1):
            st.markdown(f"**#{i}** {c}")

        st.subheader("ìš”ì•½ (ì´ ë¬¸ì¥ë§Œ ë¶„ë¥˜)")
        st.write(summary)

        # 2) ìš”ì•½ë§Œ ë¶„ë¥˜
        with st.spinner("DeBERTa ë¶„ë¥˜(ìš”ì•½) ì¶”ë¡  ì¤‘..."):
            pred = clf.predict_one(summary)
        st.markdown(f"**ì˜ˆì¸¡ ë¼ë²¨:** {pred['pred_label']}")
        st.markdown(f"**Posterior:** {max(pred['probs']):.4f}")

        # 3) í•´ì„(Attention/IG/LIME) + GPT ë¦¬í¬íŠ¸(ë‹¤êµ­ì–´)
        with st.spinner("í•´ì„ ì‚°ì¶œ(Attention/IG/LIME) + GPT ë¦¬í¬íŠ¸ ìƒì„± ì¤‘..."):
            # ê°™ì€ ì²´í¬í¬ì¸íŠ¸ë¡œ explainer êµ¬ì„±
            from MODULES.text_explain import TextAttributionEngine, GPTReporter
            import torch
            engine = TextAttributionEngine(
                hf_model=clf.model, hf_tokenizer=clf.tok, id2label=clf.id2label,
                device=("cuda" if torch.cuda.is_available() else "cpu")
            )
            try:
                payload = engine.run_all(summary)  # LIME í•„ìˆ˜, SHAP ì—†ìŒ
            except RuntimeError as e:
                st.error(str(e))
                st.stop()

            # ë¦¬í¬íŠ¸ ìƒì„± (í˜¸ê°ë„ ì „ìš© í”„ë¡¬í”„íŠ¸ê°€ text_explainer.GPTReporterì— ë°˜ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨)
            reporter = GPTReporter(
                model=os.environ.get("GPT_EXPLAIN_MODEL", "gpt-4.1-mini"),
                temperature=0.4, max_tokens=700
            )
            import asyncio
            lay_report, exp_report = asyncio.run(reporter.make_reports(payload, lang_code=lang))

        # 4) ë¦¬í¬íŠ¸ íƒ­(í† ê¸€)
        st.subheader("í•´ì„ ë¦¬í¬íŠ¸")
        tab1, tab2 = st.tabs(["ğŸ§‘â€ğŸ¤â€ğŸ§‘ ë¹„ì „ë¬¸ê°€ìš©", "ğŸ§ª ì „ë¬¸ê°€ìš©"])
        with tab1:
            st.markdown(lay_report)
        with tab2:
            st.markdown(exp_report)

        # 5) í”„ë¦¬ë·°(ì¶”ì¶œ í”„ë ˆì„ í™•ì¸)
        st.subheader("ë¯¸ë¦¬ë³´ê¸°")
        cols = st.columns(3)
        for i, img in enumerate(previews):
            with cols[i % 3]:
                st.image(img, caption=f"preview {i}", use_column_width=True)

        # (ì„ íƒ) ì›ë³¸ ë™ì˜ìƒ ë¯¸ë¦¬ë³´ê¸°
        if is_video(file_name):
            st.markdown("#### ì—…ë¡œë“œí•œ ì›ë³¸ ë™ì˜ìƒ ë¯¸ë¦¬ë³´ê¸°")
            st.video(io.BytesIO(file_bytes))

            ## ã…ã„´ã…‡ã…ã…‡ã„´